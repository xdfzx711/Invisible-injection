#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import random

# å¯¼å…¥ logger
from data_collection.utils.logger import setup_logger

class ScrapingConfig:
    """ç½‘é¡µçˆ¬å–é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: Union[str, Path] = "web_scraping_config.json"):
        self.config_file = Path(config_file)
        self.logger = setup_logger('ScrapingConfig', console_output=False)
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½çˆ¬å–é…ç½®File"""
        try:
            if self.config_file.exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                self.logger.info(f"æˆåŠŸåŠ è½½é…ç½®File: {self.config_file}")
                return config
            else:
                self.logger.info("é…ç½®Fileä¸existsï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self._get_default_config()
                
        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®FileFailed: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "request_settings": {
                "timeout": 30,
                "max_retries": 3,
                "retry_delay": 2,
                "request_delay_min": 1,
                "request_delay_max": 3,
                "max_redirects": 5
            },
            "user_agents": [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
            ],
            "scraping_rules": {
                "max_sites_per_session": 50,
                "max_pages_per_site": 5,
                "include_homepage": True,
                "include_secondary_pages": True,
                "max_page_size_mb": 10,
                "skip_binary_files": True
            },
            "secondary_page_discovery": {
                "max_links_to_check": 20,
                "preferred_link_patterns": [
                    "about", "contact", "help", "support", "news", "blog",
                    "products", "services", "company", "team", "careers"
                ],
                "exclude_patterns": [
                    "login", "register", "download", "pdf", "zip", "exe",
                    "facebook.com", "twitter.com", "instagram.com", "linkedin.com"
                ],
                "link_selection_strategy": "mixed"  # random, priority, mixed
            },
            "content_extraction": {
                "extract_text": True,
                "extract_links": True,
                "extract_images": True,
                "extract_forms": True,
                "extract_meta": True,
                "extract_scripts": False,
                "min_text_length": 10,
                "max_text_length": 10000
            },
            "output_settings": {
                "save_raw_html": True,
                "save_extracted_content": True,
                "output_format": "json",
                "compress_html": False,
                "create_summary_report": True
            },
            "safety_settings": {
                "respect_robots_txt": True,
                "check_robots_txt": False,  # ç®€åŒ–ç‰ˆæœ¬æš‚æ—¶å…³é—­
                "avoid_honeypots": True,
                "max_concurrent_requests": 1,
                "blacklisted_domains": [
                    "facebook.com", "twitter.com", "instagram.com"
                ]
            }
        }
    
    def get_request_settings(self) -> Dict[str, Any]:
        """è·å–è¯·æ±‚è®¾ç½®"""
        return self.config.get("request_settings", {})
    
    def get_random_user_agent(self) -> str:
        """è·å–éšæœºUser-Agent"""
        user_agents = self.config.get("user_agents", [])
        if user_agents:
            return random.choice(user_agents)
        return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    
    def get_scraping_rules(self) -> Dict[str, Any]:
        """è·å–çˆ¬å–è§„åˆ™"""
        return self.config.get("scraping_rules", {})
    
    def get_secondary_page_config(self) -> Dict[str, Any]:
        """è·å–äºŒçº§é¡µé¢å‘ç°é…ç½®"""
        return self.config.get("secondary_page_discovery", {})
    
    def get_content_extraction_config(self) -> Dict[str, Any]:
        """è·å–å†…å®¹æå–é…ç½®"""
        return self.config.get("content_extraction", {})
    
    def get_output_settings(self) -> Dict[str, Any]:
        """è·å–è¾“å‡ºè®¾ç½®"""
        return self.config.get("output_settings", {})
    
    def get_safety_settings(self) -> Dict[str, Any]:
        """è·å–å®‰å…¨è®¾ç½®"""
        return self.config.get("safety_settings", {})
    
    def is_domain_blacklisted(self, domain: str) -> bool:
        """CheckåŸŸåæ˜¯å¦åœ¨é»‘åå•ä¸­"""
        blacklist = self.get_safety_settings().get("blacklisted_domains", [])
        domain_lower = domain.lower()
        
        for blocked_domain in blacklist:
            if blocked_domain.lower() in domain_lower:
                return True
        return False
    
    def should_extract_content_type(self, content_type: str) -> bool:
        """Checkæ˜¯å¦åº”è¯¥æå–æŸç§ç±»å‹çš„å†…å®¹"""
        extraction_config = self.get_content_extraction_config()
        
        type_mapping = {
            "text": "extract_text",
            "links": "extract_links", 
            "images": "extract_images",
            "forms": "extract_forms",
            "meta": "extract_meta",
            "scripts": "extract_scripts"
        }
        
        setting_key = type_mapping.get(content_type)
        if setting_key:
            return extraction_config.get(setting_key, True)
        
        return True
    
    def get_request_delay(self) -> float:
        """è·å–è¯·æ±‚å»¶è¿Ÿæ—¶é—´"""
        settings = self.get_request_settings()
        min_delay = settings.get("request_delay_min", 1)
        max_delay = settings.get("request_delay_max", 3)
        return random.uniform(min_delay, max_delay)
    
    def should_include_link(self, link_text: str, link_url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«æŸä¸ªé“¾æ¥"""
        secondary_config = self.get_secondary_page_config()
        
        # Checkæ’é™¤æ¨¡å¼
        exclude_patterns = secondary_config.get("exclude_patterns", [])
        link_text_lower = link_text.lower()
        link_url_lower = link_url.lower()
        
        for pattern in exclude_patterns:
            if pattern.lower() in link_text_lower or pattern.lower() in link_url_lower:
                return False
        
        # Checkä¼˜å…ˆæ¨¡å¼
        preferred_patterns = secondary_config.get("preferred_link_patterns", [])
        for pattern in preferred_patterns:
            if pattern.lower() in link_text_lower or pattern.lower() in link_url_lower:
                return True
        
        # é»˜è®¤ç­–ç•¥
        strategy = secondary_config.get("link_selection_strategy", "mixed")
        if strategy == "priority":
            return False  # åªé€‰æ‹©ä¼˜å…ˆæ¨¡å¼åŒ¹é…çš„é“¾æ¥
        
        return True  # mixedæˆ–randomç­–ç•¥æ¥å—å…¶ä»–é“¾æ¥
    
    def save_config(self, output_file: Union[str, Path] = None) -> bool:
        """ä¿å­˜é…ç½®åˆ°File"""
        try:
            output_file = Path(output_file) if output_file else self.config_file
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"é…ç½®has beenä¿å­˜åˆ°: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜é…ç½®Failed: {e}")
            return False
    
    def update_setting(self, section: str, key: str, value: Any) -> bool:
        """æ›´æ–°é…ç½®è®¾ç½®"""
        try:
            if section not in self.config:
                self.config[section] = {}
            
            self.config[section][key] = value
            self.logger.info(f"æ›´æ–°é…ç½®: {section}.{key} = {value}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°é…ç½®Failed: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–é…ç½®Statistics"""
        return {
            "total_user_agents": len(self.config.get("user_agents", [])),
            "max_sites_per_session": self.get_scraping_rules().get("max_sites_per_session", 0),
            "max_pages_per_site": self.get_scraping_rules().get("max_pages_per_site", 0),
            "blacklisted_domains": len(self.get_safety_settings().get("blacklisted_domains", [])),
            "preferred_link_patterns": len(self.get_secondary_page_config().get("preferred_link_patterns", [])),
            "safety_enabled": self.get_safety_settings().get("respect_robots_txt", False)
        }
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        stats = self.get_statistics()
        
        print("\n" + "="*50)
        print("ğŸ”§ ç½‘é¡µçˆ¬å–é…ç½®æ‘˜è¦")
        print("="*50)
        print(f"ğŸŒ User-Agentæ•°é‡: {stats['total_user_agents']}")
        print(f"ğŸ“Š æœ€å¤§ç½‘ç«™æ•°: {stats['max_sites_per_session']}")
        print(f"ğŸ“„ æ¯ç«™æœ€å¤§é¡µé¢æ•°: {stats['max_pages_per_site']}")
        print(f"ğŸš« é»‘åå•åŸŸå: {stats['blacklisted_domains']} ä¸ª")
        print(f"â­ ä¼˜å…ˆé“¾æ¥æ¨¡å¼: {stats['preferred_link_patterns']} ä¸ª")
        print(f"ğŸ›¡ï¸  å®‰å…¨æ¨¡å¼: {'å¯ç”¨' if stats['safety_enabled'] else 'ç¦ç”¨'}")
        print("="*50)
