#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Reddit Data Parsingå™¨
ä»Reddit JSONFileä¸­æå–å¸–å­å’Œè¯„è®ºæ–‡æœ¬
"""

import json
from pathlib import Path
from typing import Dict, Any, List
import html
import re
from datetime import datetime

import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from data_parsing.base_parser import BaseParser
from data_parsing.utils import FileUtils


class RedditParser(BaseParser):
    """Reddit Data Parsingå™¨ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰"""
    
    def __init__(self, enable_interference_filter: bool = True, filter_config: Dict[str, Any] = None):
        super().__init__('reddit', enable_interference_filter, filter_config)
    
    def decode_text(self, text: str) -> str:
        """åŸºç¡€æ–‡æœ¬è§£ç å¤„ç†"""
        if not text:
            return ""
        
        # HTMLå®ä½“è§£ç 
        text = html.unescape(text)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        # åº”ç”¨å¹²æ‰°å­—ç¬¦è¿‡æ»¤å™¨
        text = self._process_extracted_text(text)
        
        return text
    
    def extract_text_content(self, post: Dict) -> str:
        """åˆå¹¶å¸–å­çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
        text_parts = []
        
        if post.get('title'):
            text_parts.append(post['title'])
        
        if post.get('selftext'):
            text_parts.append(post['selftext'])
        
        return ' '.join(text_parts).strip()
    
    def process_posts(self, submissions: List[Dict], subreddit_name: str, source_file: str) -> Dict[str, Any]:
        """å¤„ç†å¸–å­æ•°æ®ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰"""
        processed_posts = []
        
        for post in submissions:
            # åªä¿ç•™æœ‰ç”¨å­—æ®µå¹¶è§£ç 
            cleaned_post = {
                'id': post['id'],
                'title': self.decode_text(post.get('title', '')),
                'selftext': self.decode_text(post.get('selftext', '')),
                'score': post.get('score', 0),
                'url': post.get('url', ''),
                'permalink': post.get('permalink', '')
            }
            
            # æ·»åŠ å¤„ç†åçš„å…ƒæ•°æ®
            cleaned_post['text_content'] = self.extract_text_content(cleaned_post)
            cleaned_post['content_length'] = len(cleaned_post['text_content'])
            
            processed_posts.append(cleaned_post)
        
        # æ„å»ºè¾“å‡ºæ•°æ®ï¼ˆä¸æ—§æ ¼å¼å®Œå…¨ç›¸åŒï¼‰
        output_data = {
            "parsing_info": {
                "subreddit": subreddit_name,
                "timestamp": datetime.now().isoformat(),
                "total_posts": len(processed_posts),
                "source_file": source_file
            },
            "posts": processed_posts
        }
        
        return output_data
    
    def process_comments(self, comments: List[Dict], subreddit_name: str, source_file: str) -> Dict[str, Any]:
        """å¤„ç†è¯„è®ºæ•°æ®ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰"""
        processed_comments = []
        
        for comment in comments:
            # åªä¿ç•™æœ‰ç”¨å­—æ®µå¹¶è§£ç 
            cleaned_comment = {
                'id': comment['id'],
                'submission_id': comment.get('submission_id', ''),
                'body': self.decode_text(comment.get('body', '')),
                'score': comment.get('score', 0),
                'permalink': comment.get('permalink', '')
            }
            
            # æ·»åŠ å¤„ç†åçš„å…ƒæ•°æ®
            cleaned_comment['text_content'] = cleaned_comment['body']
            cleaned_comment['content_length'] = len(cleaned_comment['text_content'])
            
            processed_comments.append(cleaned_comment)
        
        # æ„å»ºè¾“å‡ºæ•°æ®ï¼ˆä¸æ—§æ ¼å¼å®Œå…¨ç›¸åŒï¼‰
        output_data = {
            "parsing_info": {
                "subreddit": subreddit_name,
                "timestamp": datetime.now().isoformat(),
                "total_comments": len(processed_comments),
                "source_file": source_file
            },
            "comments": processed_comments
        }
        
        return output_data
    
    def extract_subreddit_name(self, filename: str) -> str:
        """ä»Fileåæå–subredditåç§°"""
        # chatgpt_promptDesign_data.json -> chatgpt_promptDesign
        # unicode_data.json -> unicode
        base_name = filename.replace('_data.json', '')
        return base_name
    
    def parse_file(self, file_path: Path) -> bool:
        """
        è§£æå•ä¸ªReddit JSONFile
        è¿”å›å¸ƒå°”å€¼è¡¨ç¤ºæˆåŠŸ/Failed
        """
        self.logger.info(f"Parsing Reddit file: {file_path}")
        print(f"ğŸ” å¤„ç†File: {file_path.name}")
        
        try:
            # è¯»å–JSONFile
            content = FileUtils.safe_read_file(file_path)
            data = json.loads(content)
            
            # æå–subredditåç§°
            subreddit_name = self.extract_subreddit_name(file_path.name)
            
            # å¤„ç†å¸–å­
            if 'submissions' in data and data['submissions']:
                posts_data = self.process_posts(data['submissions'], subreddit_name, file_path.name)
                posts_filename = f"{subreddit_name}_posts_parsed.json"
                posts_path = self.output_dir / posts_filename
                self.save_parsed_data(posts_data, posts_path)
                
                print(f"   ğŸ“‹ å¤„ç†å¸–å­: {len(data['submissions'])} ä¸ª")
            
            # å¤„ç†è¯„è®º
            if 'comments' in data and data['comments']:
                comments_data = self.process_comments(data['comments'], subreddit_name, file_path.name)
                comments_filename = f"{subreddit_name}_comments_parsed.json"
                comments_path = self.output_dir / comments_filename
                self.save_parsed_data(comments_data, comments_path)
                
                print(f"   ğŸ’¬ å¤„ç†è¯„è®º: {len(data['comments'])} ä¸ª")
            
            self.logger.info(f"Successfully parsed {file_path.name}")
            return True
            
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON decode error in {file_path}: {e}")
            print(f"   âŒ JSONæ ¼å¼Error: {e}")
            return False
        except Exception as e:
            self.logger.error(f"Error parsing {file_path}: {e}")
            print(f"   âŒ è§£æError: {e}")
            return False
    
    def _get_files_to_parse(self, directory: Path) -> List[Path]:
        """è·å–æ‰€æœ‰Reddit JSONFile"""
        return list(directory.glob('*_data.json'))
    
    def parse_directory(self, directory: Path = None) -> List[Dict[str, Any]]:
        """
        è§£ææ•´ä¸ªdirectory
        """
        if directory is None:
            directory = self.input_dir
        
        if not directory.exists():
            self.logger.warning(f"Directory not found: {directory}")
            return []
        
        files = self._get_files_to_parse(directory)
        self.logger.info(f"Found {len(files)} files to parse in {directory}")
        print(f"\næ‰¾åˆ° {len(files)} ä¸ªRedditæ•°æ®File")
        
        self.stats['total_files'] = len(files)
        
        subreddits = []
        for i, file_path in enumerate(files, 1):
            print(f"\n[{i}/{len(files)}]", end=' ')
            success = self.parse_file(file_path)
            
            if success:
                self.stats['successful_files'] += 1
                subreddit_name = self.extract_subreddit_name(file_path.name)
                subreddits.append(subreddit_name)
                print(f"âœ… {file_path.name} å¤„ç†Completed")
            else:
                self.stats['failed_files'] += 1
                print(f"âŒ {file_path.name} å¤„ç†Failed")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        if subreddits:
            self._generate_summary_report(subreddits)
        
        # æ³¨æ„ï¼šè¿™é‡Œè¿”å›ç©ºåˆ—è¡¨ï¼Œå› ä¸ºç»“æœhas beenç»åˆ†åˆ«ä¿å­˜åˆ°ä¸åŒFile
        # ä¸éœ€è¦æ‰¹é‡æ±‡æ€»
        return []
    
    def _generate_summary_report(self, subreddits: List[str]):
        """ç”Ÿæˆè§£ææ±‡æ€»æŠ¥å‘Š"""
        summary = {
            "processing_info": {
                "timestamp": datetime.now().isoformat(),
                "input_directory": str(self.input_dir),
                "output_directory": str(self.output_dir)
            },
            "statistics": {
                "processed_files": self.stats['successful_files'],
                "subreddits": list(set(subreddits))
            },
            "output_files": []
        }
        
        # åˆ—å‡ºç”Ÿæˆçš„File
        for subreddit in set(subreddits):
            summary["output_files"].extend([
                f"{subreddit}_posts_parsed.json",
                f"{subreddit}_comments_parsed.json"
            ])
        
        summary_path = self.output_dir / "reddit_parsing_summary.json"
        self.save_parsed_data(summary, summary_path)
        print(f"\nğŸ’¾ æ±‡æ€»æŠ¥å‘Šhas beenä¿å­˜: reddit_parsing_summary.json")
