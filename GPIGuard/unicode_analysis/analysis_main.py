#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Unicode Threat Analysis Main Program
Extract characters from parsed structured data and detect specified Unicode threat characters
"""

import time
import argparse
from pathlib import Path
from typing import Dict, List, Any, Union
import sys
import os

# Add project root directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import project tools
from data_collection.utils.logger import setup_logger

# Fix relative import issues
try:
    from .character_extractor import CharacterExtractor
    from .identifier_status_config import IdentifierStatusConfig
    from .identifier_status_detector import IdentifierStatusDetector
    from .homograph_config import HomographConfig
    from .homograph_detector import HomographDetector
    from .unicode_type_classifier import UnicodeTypeClassifier
    from .threat_formatter import ThreatFormatter
    from .comparison_report_generator import ComparisonReportGenerator
    from .threat_report_converter import ThreatReportConverter
except ImportError:
    # If relative import fails, use absolute import
    from character_extractor import CharacterExtractor
    from identifier_status_config import IdentifierStatusConfig
    from identifier_status_detector import IdentifierStatusDetector
    from homograph_config import HomographConfig
    from homograph_detector import HomographDetector
    from unicode_type_classifier import UnicodeTypeClassifier
    from threat_formatter import ThreatFormatter
    from comparison_report_generator import ComparisonReportGenerator
    from threat_report_converter import ThreatReportConverter

class UnicodeAnalysisManager:
    """Unicode Analysis Manager - Simplified version based on identifier status"""

    def __init__(self, output_dir: Union[str, Path] = None,
                 lookup_file: Union[str, Path] = None,
                 data_sources: List[str] = None,
                 force_extract: bool = False,
                 sample_size: int = None,
                 enable_homograph: bool = True):

        # Set default paths (relative to testscan directory)
        if output_dir is None:
            # ä»unicode_analysisdirectoryå‘ä¸Šæ‰¾åˆ°testscandirectory
            current_dir = Path(__file__).parent
            testscan_dir = current_dir.parent
            output_dir = testscan_dir / "testscan_data"

        if lookup_file is None:
            # Lookup table file is in testscan_data/unicode_analysis directory
            current_dir = Path(__file__).parent
            testscan_dir = current_dir.parent
            lookup_file = testscan_dir / "testscan_data" / "unicode_analysis" / "identifier_status_lookup.json"

        self.output_dir = Path(output_dir)
        self.force_extract = force_extract
        self.logger = setup_logger('UnicodeAnalysisManager', 'unicode_analysis.log')

        # Set data sources to process
        self.data_sources = data_sources or ['json', 'csv', 'xml', 'html', 'reddit', 'twitter', 'github', 'godofprompt']
        self.logger.info(f"Enabled data sources: {', '.join(self.data_sources)}")
        if force_extract:
            self.logger.info("å¼ºåˆ¶é‡æ–°æå–å­—ç¬¦æ¨¡å¼has beenå¯ç”¨")

        # Initialize configuration
        self.config = IdentifierStatusConfig(lookup_file)

        # Create main output directory (must be defined before use)
        self.analysis_output_dir = self.output_dir / "unicode_analysis"
        self.analysis_output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize components
        self.character_extractor = CharacterExtractor(output_dir, self.data_sources)
        self.restriction_detector = IdentifierStatusDetector(self.config, output_dir, self.data_sources)

        # Initialize homoglyph character detector (optional)
        self.enable_homograph = enable_homograph
        self.homograph_detector = None
        confusables_file = self.analysis_output_dir / "unicode_confusables.json"
        
        if enable_homograph:
            try:
                # Find confusables data file
                if confusables_file.exists():
                    homograph_config = HomographConfig(confusables_file)
                    self.homograph_detector = HomographDetector(homograph_config, output_dir, self.data_sources)
                    self.logger.info("Homoglyph Characteræ£€æµ‹å™¨has beenå¯ç”¨")
                else:
                    self.logger.warning(f"Confusables data file not found: {confusables_file}")
                    self.logger.warning("Homoglyph detection will be skipped")
                    self.enable_homograph = False
            except Exception as e:
                self.logger.error(f"Failed to initialize homoglyph detector: {e}")
                self.enable_homograph = False
        else:
            self.logger.info("Homoglyph detection is disabled")
        
        # åˆå§‹åŒ–æ–°æ ¼å¼åŒ–ç»„ä»¶
        self.logger.info("Initializing threat formatting component...")
        self.unicode_classifier = UnicodeTypeClassifier(confusables_file if confusables_file.exists() else None)
        self.threat_formatter = ThreatFormatter()
        self.logger.info("Threat formatting component initialized")

        # Initializing comparison report generator
        self.comparison_report_generator = ComparisonReportGenerator()
        self.logger.info("Comparison report generator initialized")
        
        # Initializing threat report converter
        self.threat_converter = ThreatReportConverter(logger=self.logger)
        self.logger.info("Threat report converter initialized")
    
    def analyze_unicode_restrictions(self, parsed_data_dir: Union[str, Path] = None) -> Dict[str, Any]:
        """Perform Unicode identifier status analysis"""
        self.logger.info("Starting Unicode identifier status analysis...")

        if parsed_data_dir is None:
            # Use unified parsed_data directory, let character_extractor find subdirectories
            parsed_data_dir = self.output_dir / "parsed_data"
            self.logger.info(f"Using parsed data directory: {parsed_data_dir}")

        parsed_data_dir = Path(parsed_data_dir)

        if not parsed_data_dir.exists():
            self.logger.error(f"Parsed data directory does not exist: {parsed_data_dir}")
            return {"error": "Parsed data directory does not exist"}
        
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½å­—ç¬¦æå–ï¼ˆè‡ªåŠ¨Checkhas beenæœ‰Fileï¼‰
            self.logger.info("Step 1: Intelligent character extraction...")
            all_characters = self.character_extractor.extract_from_parsed_data_smart(
                parsed_data_dir,
                force_extract=self.force_extract
            )

            if not all_characters:
                self.logger.warning("No characters extracted")

                # Provide detailed diagnostic information
                diagnostic_info = self._diagnose_extraction_failure(parsed_data_dir)
                error_message = f"No characters extractedã€‚{diagnostic_info}"

                self.logger.error(error_message)
                return {"error": error_message}

            # ç¬¬äºŒæ­¥ï¼šæ£€æµ‹å—é™å­—ç¬¦
            self.logger.info("Step 2: Detect restricted characters...")
            restriction_detections = self.restriction_detector.detect_restrictions_in_characters(all_characters)

            # ç¬¬ä¸‰æ­¥ï¼šæ£€æµ‹Homoglyph Characterï¼ˆå¯é€‰ï¼‰
            homograph_detections = []
            if self.enable_homograph and self.homograph_detector:
                self.logger.info("Step 3: Detect homoglyph characters...")
                homograph_detections = self.homograph_detector.detect_homographs_in_characters(all_characters)
            else:
                self.logger.info("Step 3: Skip homoglyph detection")

            # ç¬¬å››æ­¥ï¼šGenerate new format threat reports
            self.logger.info("Step 4: Generate new format threat reports...")
            formatted_reports = self._generate_formatted_reports(
                restriction_detections, homograph_detections
            )

            # æ–°å¢æ­¥éª¤ï¼šç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
            if homograph_detections:
                self.logger.info("Step 4.5: Generate homoglyph comparison reports...")
                comparison_reports = self.comparison_report_generator.generate_reports(
                    all_characters, homograph_detections
                )
                if comparison_reports:
                    # æŒ‰æ•°æ®æºç±»å‹åˆ†ç»„ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
                    self.comparison_report_generator.save_reports_by_source(
                        comparison_reports, self.output_dir
                    )
                    self.logger.info(f"Comparison reports saved by data source type to {self.output_dir}/threat_detection_*/ directory")

            # ç¬¬äº”æ­¥ï¼šGenerate analysis results
            end_time = time.time()
            analysis_result = self._generate_analysis_result(
                all_characters, restriction_detections, homograph_detections, start_time, end_time
            )
            
            # æ·»åŠ æ–°æ ¼å¼æŠ¥å‘Šä¿¡æ¯åˆ°åˆ†æç»“æœ
            analysis_result["formatted_reports"] = {
                "total_threats": len(formatted_reports),
                "reports_generated": True
            }

            self.logger.info("Unicode identifier status analysis completed")
            return analysis_result

        except Exception as e:
            self.logger.error(f"Unicode identifier status analysis failed: {e}")
            return {"error": f"åˆ†æFailed: {e}"}
    
    def _generate_formatted_reports(self, 
                                    restriction_detections: List[Dict[str, Any]],
                                    homograph_detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Generate new format threat reports
        
        Args:
            restriction_detections: Restricted character detection results
            homograph_detections: Homoglyph character detection results
        
        Returns:
            List of formatted threat reports
        """
        self.logger.info("å¼€å§‹ç”Ÿæˆæ–°æ ¼å¼å¨èƒæŠ¥å‘Š...")
        
        # Merge all detection results
        all_detections = []
        
        # Add restricted character detection (mainly zero_width, bidi, etc.)
        all_detections.extend(restriction_detections)
        
        # Add homoglyph character detection (confusables)
        all_detections.extend(homograph_detections)
        
        if not all_detections:
            self.logger.info("No threat characters detected")
            return []
        
        self.logger.info(f"Total detected {len(all_detections)} threat characters")
        
        # Use formatter to generate reports
        formatted_reports = self.threat_formatter.generate_threat_reports(
            all_detections,
            self.unicode_classifier
        )
        
        self.logger.info(f"Generated {len(formatted_reports)} threat reports")
        
        # æŒ‰æ•°æ®æºä¿å­˜æŠ¥å‘Š
        self._save_formatted_reports_by_source(formatted_reports)
        
        return formatted_reports
    
    def _save_formatted_reports_by_source(self, reports: List[Dict[str, Any]]):
        """Save formatted reports by data source"""
        # æŒ‰æ•°æ®æºåˆ†ç»„
        reports_by_source = {}
        for report in reports:
            source_type = report["source_info"].get("source_type", "unknown")
            if source_type not in reports_by_source:
                reports_by_source[source_type] = []
            reports_by_source[source_type].append(report)
        
        # ä¸ºæ¯ä¸ªæ•°æ®æºä¿å­˜æŠ¥å‘Š
        for source_type, source_reports in reports_by_source.items():
            output_dir = self.output_dir / f"threat_detection_{source_type}"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜JSONæ ¼å¼
            json_file = output_dir / "formatted_threats.json"
            self.threat_formatter.save_formatted_reports(
                source_reports, 
                json_file,
                include_metadata=True
            )
            
            # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
            stats = self.threat_formatter.generate_summary_statistics(source_reports)
            stats_file = output_dir / "threat_summary_by_type.json"
            
            import json
            try:
                with open(stats_file, 'w', encoding='utf-8') as f:
                    json.dump(stats, f, ensure_ascii=False, indent=2)
                self.logger.info(f"ç»Ÿè®¡æ‘˜è¦has beenä¿å­˜: {stats_file}")
            except Exception as e:
                self.logger.error(f"ä¿å­˜ç»Ÿè®¡æ‘˜è¦Failed: {e}")
            
            # Convert to standard formatå¹¶åˆ†ç¦»BIDI threats
            self.logger.info(f"å¼€å§‹è½¬æ¢{source_type}å¨èƒæŠ¥å‘Šä¸ºæ ‡å‡†æ ¼å¼...")
            try:
                conversion_stats = self.threat_converter.convert_formatted_threats(json_file, output_dir)
                if conversion_stats.get('conversion_success', False):
                    self.logger.info(f"{source_type}å¨èƒæŠ¥å‘ŠConversion completed:")
                    self.logger.info(f"  - BIDI threats: {conversion_stats['bidi_converted']} entries")
                    self.logger.info(f"  - Other threats: {conversion_stats['non_bidi_converted']} entries")
                else:
                    self.logger.error(f"{source_type}å¨èƒæŠ¥å‘Šè½¬æ¢Failed: {conversion_stats.get('error', 'æœªçŸ¥Error')}")
            except Exception as e:
                self.logger.error(f"è½¬æ¢{source_type}å¨èƒæŠ¥å‘Šæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
    
    def _generate_analysis_result(self, all_characters: List[Dict[str, Any]],
                                restriction_detections: List[Dict[str, Any]],
                                homograph_detections: List[Dict[str, Any]],
                                start_time: float, end_time: float) -> Dict[str, Any]:
        """Generate analysis results"""
        
        # åŸºæœ¬ç»Ÿè®¡
        char_summary = self.character_extractor.get_character_summary(all_characters)
        
        # å—é™å­—ç¬¦ç»Ÿè®¡
        restriction_stats = self._calculate_restriction_stats(restriction_detections)

        # Homoglyph Characterç»Ÿè®¡
        homograph_stats = self._calculate_homograph_stats(homograph_detections)

        # Fileç»Ÿè®¡
        file_stats = self._calculate_file_stats(all_characters, restriction_detections, homograph_detections)

        # æ£€æµ‹å™¨ç»Ÿè®¡
        detector_stats = self.restriction_detector.get_detection_statistics()
        homograph_detector_stats = self.homograph_detector.get_detection_statistics() if self.homograph_detector else {}

        analysis_result = {
            "analysis_info": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_seconds": end_time - start_time,
                "analysis_timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(end_time)),
                "output_directory": str(self.restriction_detector.output_dir.relative_to(self.output_dir.parent)),
                "analysis_type": "identifier_status"
            },
            "character_extraction": {
                "total_characters": char_summary["total_characters"],
                "unique_characters": char_summary["unique_characters"],
                "source_types": char_summary["source_types"]
            },
            "restriction_detection": {
                "total_restrictions": len(restriction_detections),
                "restriction_rate": len(restriction_detections) / char_summary["total_characters"] if char_summary["total_characters"] > 0 else 0,
                "allowed_characters": detector_stats["allowed_characters_found"],
                "restricted_characters": detector_stats["restricted_characters_found"],
                "detection_types": restriction_stats["detection_types"],
                "top_restricted_chars": restriction_stats["top_restricted_chars"]
            },
            "homograph_detection": {
                "enabled": self.enable_homograph,
                "total_homographs": len(homograph_detections),
                "homograph_rate": len(homograph_detections) / char_summary["total_characters"] if char_summary["total_characters"] > 0 else 0,
                "confusable_types": homograph_stats["confusable_types"],
                "top_confusable_chars": homograph_stats["top_confusable_chars"],
                "detection_time": homograph_detector_stats.get("detection_time", 0)
            },
            "file_analysis": file_stats,
            "config_info": {
                "total_allowed_characters": len(self.config.allowed_characters),
                "detection_settings": self.config.get_detection_settings(),
                "config_status": self.config.get_statistics()["config_status"]
            }
        }
        
        return analysis_result

    def _calculate_homograph_stats(self, homograph_detections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—Homoglyph CharacterStatistics"""

        confusable_types = {}
        confusable_chars = {}

        for detection in homograph_detections:
            # æ··æ·†ç±»å‹ç»Ÿè®¡
            confusable_type = detection.get("confusable_type", "unknown")
            confusable_types[confusable_type] = confusable_types.get(confusable_type, 0) + 1

            # æ··æ·†å­—ç¬¦ç»Ÿè®¡
            character = detection.get("character", "")
            unicode_point = detection.get("unicode_point", "")
            char_key = f"{character} ({unicode_point})"
            confusable_chars[char_key] = confusable_chars.get(char_key, 0) + 1

        # è·å–å‰10ä¸ªæœ€å¸¸è§çš„æ··æ·†å­—ç¬¦
        top_confusable_chars = sorted(confusable_chars.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            "confusable_types": confusable_types,
            "top_confusable_chars": top_confusable_chars,
            "unique_confusable_chars": len(confusable_chars)
        }

    def _calculate_restriction_stats(self, restriction_detections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—å—é™å­—ç¬¦Statistics"""

        detection_types = {}
        restricted_chars = {}
        char_counts = {}
        
        for detection in restriction_detections:
            # æ£€æµ‹ç±»å‹ç»Ÿè®¡
            detection_type = detection.get("detection_info", {}).get("detection_type", "unknown")
            detection_types[detection_type] = detection_types.get(detection_type, 0) + 1

            # å—é™å­—ç¬¦ç»Ÿè®¡
            char_key = detection["unicode_point"]
            if char_key not in char_counts:
                char_counts[char_key] = {
                    "character": detection["character"],
                    "unicode_point": detection["unicode_point"],
                    "name": detection.get("name", "UNKNOWN"),
                    "status": detection.get("status", "Restricted"),
                    "category": detection.get("category", "Unknown"),
                    "count": 0
                }
            char_counts[char_key]["count"] += 1

        # è·å–æœ€å¸¸è§çš„å—é™å­—ç¬¦
        top_restricted_chars = sorted(char_counts.values(), key=lambda x: x["count"], reverse=True)[:10]

        return {
            "detection_types": detection_types,
            "top_restricted_chars": top_restricted_chars
        }
    
    def _calculate_file_stats(self, all_characters: List[Dict[str, Any]],
                            restriction_detections: List[Dict[str, Any]],
                            homograph_detections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—FileStatistics"""
        
        # æ‰€æœ‰Fileç»Ÿè®¡
        all_files = {}
        for char in all_characters:
            file_path = char["source_info"]["file_path"]
            if file_path not in all_files:
                all_files[file_path] = {
                    "file_path": file_path,
                    "file_name": char["source_info"]["file_name"],
                    "source_type": char["source_info"]["source_type"],
                    "total_characters": 0,
                    "restrictions_found": 0,
                    "homographs_found": 0
                }
            all_files[file_path]["total_characters"] += 1

        # å—é™å­—ç¬¦Fileç»Ÿè®¡
        for detection in restriction_detections:
            file_path = detection["source_info"]["file_path"]
            if file_path in all_files:
                all_files[file_path]["restrictions_found"] += 1

        # Homoglyph CharacterFileç»Ÿè®¡
        for detection in homograph_detections:
            file_path = detection["source_info"]["file_path"]
            if file_path in all_files:
                all_files[file_path]["homographs_found"] += 1

        # è®¡ç®—æ¯”ç‡
        for file_info in all_files.values():
            file_info["restriction_rate"] = file_info["restrictions_found"] / file_info["total_characters"] if file_info["total_characters"] > 0 else 0
            file_info["homograph_rate"] = file_info["homographs_found"] / file_info["total_characters"] if file_info["total_characters"] > 0 else 0

        # æŒ‰å—é™å­—ç¬¦æ•°é‡æ’åº
        files_by_restrictions = sorted(all_files.values(), key=lambda x: x["restrictions_found"], reverse=True)

        return {
            "total_files": len(all_files),
            "files_with_restrictions": len([f for f in all_files.values() if f["restrictions_found"] > 0]),
            "files_with_homographs": len([f for f in all_files.values() if f["homographs_found"] > 0]),
            "files_by_restriction_count": files_by_restrictions[:10],  # å‰10ä¸ªå—é™å­—ç¬¦æœ€å¤šçš„File
            "source_type_distribution": self._get_source_type_distribution(all_files.values())
        }
    
    def _get_source_type_distribution(self, file_infos) -> Dict[str, Dict[str, int]]:
        """è·å–æŒ‰æºç±»å‹çš„åˆ†å¸ƒç»Ÿè®¡"""
        distribution = {}
        
        for file_info in file_infos:
            source_type = file_info["source_type"]
            if source_type not in distribution:
                distribution[source_type] = {
                    "total_files": 0,
                    "files_with_restrictions": 0,
                    "total_characters": 0,
                    "total_restrictions": 0
                }

            distribution[source_type]["total_files"] += 1
            distribution[source_type]["total_characters"] += file_info["total_characters"]
            distribution[source_type]["total_restrictions"] += file_info["restrictions_found"]

            if file_info["restrictions_found"] > 0:
                distribution[source_type]["files_with_restrictions"] += 1
        
        return distribution

    def _diagnose_extraction_failure(self, parsed_data_dir: Path) -> str:
        """è¯Šæ–­å­—ç¬¦æå–Failedçš„åŸå› """
        diagnostic_messages = []

        # CheckParsed data directory
        if not parsed_data_dir.exists():
            diagnostic_messages.append(f"Parsed data directory does not exist: {parsed_data_dir}")
            return " ".join(diagnostic_messages)

        # Checkå„æ•°æ®æºdirectory
        source_handlers = {
            'json': 'json_analysis',
            'csv': 'csv_analysis',
            'xml': 'xml_analysis',
            'html': 'html_analysis',
            'reddit': 'reddit_analysis',
            'twitter': 'twitter_analysis',
            'github': 'github_analysis'
        }

        missing_dirs = []
        empty_dirs = []
        existing_dirs = []

        for source_type in self.data_sources:
            if source_type in source_handlers:
                dir_name = source_handlers[source_type]
            else:
                # åŠ¨æ€æ•°æ®æºdirectoryæ¨æ–­
                dir_name = f"{source_type}_analysis"
                
            source_dir = parsed_data_dir / dir_name

            if not source_dir.exists():
                missing_dirs.append(f"{source_type}({dir_name})")
            elif not any(source_dir.iterdir()):
                empty_dirs.append(f"{source_type}({dir_name})")
            else:
                existing_dirs.append(f"{source_type}({dir_name})")

        # Checkå­—ç¬¦æå–File
        char_output_dir = self.output_dir / "unicode_analysis" / "character_extraction"
        extraction_file_status = []
        large_files = []

        for source_type in self.data_sources:
            extraction_file = char_output_dir / f"character_extraction_{source_type}.json"
            if extraction_file.exists():
                try:
                    size = extraction_file.stat().st_size
                    size_gb = size / (1024 * 1024 * 1024)

                    if size == 0:
                        extraction_file_status.append(f"{source_type}(Fileä¸ºç©º)")
                    elif size_gb > 1.0:
                        extraction_file_status.append(f"{source_type}(Fileexists,{size_gb:.2f}GB)")
                        if size_gb > 8.0:
                            large_files.append(f"{source_type}({size_gb:.2f}GB)")
                    else:
                        size_mb = size / (1024 * 1024)
                        extraction_file_status.append(f"{source_type}(Fileexists,{size_mb:.1f}MB)")
                except Exception as e:
                    extraction_file_status.append(f"{source_type}(FileError:{e})")
            else:
                extraction_file_status.append(f"{source_type}(Fileä¸exists)")

        # æ„å»ºè¯Šæ–­æ¶ˆæ¯
        if missing_dirs:
            diagnostic_messages.append(f"ç¼ºå¤±æ•°æ®æºdirectory: {', '.join(missing_dirs)}")

        if empty_dirs:
            diagnostic_messages.append(f"ç©ºæ•°æ®æºdirectory: {', '.join(empty_dirs)}")

        if existing_dirs:
            diagnostic_messages.append(f"existsæ•°æ®æºdirectory: {', '.join(existing_dirs)}")

        diagnostic_messages.append(f"å­—ç¬¦æå–FileçŠ¶æ€: {', '.join(extraction_file_status)}")

        # æä¾›å»ºè®®
        suggestions = []
        if missing_dirs or empty_dirs:
            suggestions.append("è¯·å…ˆè¿è¡Œæ•°æ®è§£ææ­¥éª¤ç”Ÿæˆè§£ææ•°æ®")

        if not existing_dirs:
            suggestions.append("è¯·Checkæ•°æ®æºé…ç½®å’ŒParsed data directory")

        if large_files:
            suggestions.append(f"æ£€æµ‹åˆ°è¶…å¤§File({', '.join(large_files)})å¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³")
            suggestions.append("å»ºè®®: 1) å¢åŠ ç³»ç»Ÿå†…å­˜ 2) ä½¿ç”¨ --force-extract é‡æ–°ç”ŸæˆFile 3) åˆ†æ‰¹å¤„ç†æ•°æ®")
        else:
            suggestions.append("å¯ä»¥ä½¿ç”¨ --force-extract å‚æ•°å¼ºåˆ¶é‡æ–°æå–å­—ç¬¦")

        if suggestions:
            diagnostic_messages.append(f"å»ºè®®: {'; '.join(suggestions)}")

        return " ".join(diagnostic_messages)

    # æ³¨é‡Šï¼šåˆ é™¤_save_analysis_resultæ–¹æ³•ï¼Œä¸å†ç”Ÿæˆunicode_analysis_result.json
    # def _save_analysis_result(self, analysis_result: Dict[str, Any]):
    #     """ä¿å­˜åˆ†æç»“æœ"""
    #
    #     import json
    #
    #     result_file = self.analysis_output_dir / "unicode_analysis_result.json"
    #     try:
    #         with open(result_file, 'w', encoding='utf-8') as f:
    #             json.dump(analysis_result, f, ensure_ascii=False, indent=2)
    #         self.logger.info(f"åˆ†æç»“æœhas beenä¿å­˜: {result_file}")
    #     except Exception as e:
    #         self.logger.error(f"ä¿å­˜åˆ†æç»“æœFailed: {e}")

    def print_analysis_summary(self, analysis_result: Dict[str, Any]):
        """æ‰“å°åˆ†ææ‘˜è¦"""

        print("\n" + "="*70)
        print("ğŸ” Unicode identifier status analysis completedæ‘˜è¦")
        print("="*70)

        # åŸºæœ¬ä¿¡æ¯
        char_info = analysis_result["character_extraction"]
        restriction_info = analysis_result["restriction_detection"]
        file_info = analysis_result["file_analysis"]

        print(f"â±ï¸  åˆ†æè€—æ—¶: {analysis_result['analysis_info']['duration_seconds']:.2f} ç§’")
        print(f"ğŸ“Š å­—ç¬¦åˆ†æ: {char_info['total_characters']} ä¸ªå­—ç¬¦ ({char_info['unique_characters']} ä¸ªå”¯ä¸€å­—ç¬¦)")
        print(f"ğŸ“ Fileåˆ†æ: {file_info['total_files']} ä¸ªFile")

        # Restricted character detection results
        print(f"\nâœ… æ ‡è¯†ç¬¦çŠ¶æ€æ£€æµ‹:")
        print(f"   âœ… å…è®¸å­—ç¬¦: {restriction_info['allowed_characters']} ä¸ª")
        print(f"   âŒ å—é™å­—ç¬¦: {restriction_info['restricted_characters']} ä¸ª")
        print(f"   ğŸ“ˆ å—é™ç‡: {restriction_info['restriction_rate']:.4f} ({restriction_info['restriction_rate']*100:.2f}%)")
        print(f"   ğŸ“ æ¶‰åŠFile: {file_info['files_with_restrictions']}/{file_info['total_files']} ä¸ª")

        if restriction_info["total_restrictions"] > 0:
            print(f"\nğŸ“Š æ£€æµ‹ç±»å‹åˆ†å¸ƒ:")
            for detection_type, count in restriction_info["detection_types"].items():
                print(f"   â€¢ {detection_type}: {count} ä¸ª")

            if restriction_info["top_restricted_chars"]:
                print(f"\nğŸ” æœ€å¸¸è§å—é™å­—ç¬¦:")
                for i, restricted_char in enumerate(restriction_info["top_restricted_chars"][:5]):
                    print(f"   {i+1}. '{restricted_char['character']}' ({restricted_char['unicode_point']}) - {restricted_char['count']} æ¬¡")

        else:
            print(f"\nâœ… æ ‡è¯†ç¬¦çŠ¶æ€æ£€æµ‹: æ‰€æœ‰å­—ç¬¦å‡ä¸ºå…è®¸çŠ¶æ€")
        
        # æ–°æ ¼å¼æŠ¥å‘Šä¿¡æ¯
        if "formatted_reports" in analysis_result and analysis_result["formatted_reports"]["reports_generated"]:
            print(f"\nğŸ“ æ–°æ ¼å¼å¨èƒæŠ¥å‘Š:")
            print(f"   ğŸ“„ å¨èƒæŠ¥å‘Šæ•°: {analysis_result['formatted_reports']['total_threats']} entries")
            print(f"   âœ… has beenç”Ÿæˆæ–°æ ¼å¼æŠ¥å‘ŠFile:")
            print(f"      â€¢ formatted_threats.json (JSONæ ¼å¼)")
            print(f"      â€¢ threat_summary_by_type.json (æŒ‰ç±»å‹ç»Ÿè®¡)")
        
        print(f"\nğŸ“‚ Output directory: {self.analysis_output_dir}")
        print("="*70)

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Unicodeæ ‡è¯†ç¬¦çŠ¶æ€åˆ†æå·¥å…· - ä»æŒ‡å®šæ•°æ®æºæå–å­—ç¬¦å¹¶æ£€æµ‹å—é™å­—ç¬¦",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python analysis_main.py                    # åˆ†ææ‰€æœ‰æ•°æ®æº
  python analysis_main.py --csv              # åªåˆ†æCSVæ•°æ®
  python analysis_main.py --reddit           # åªåˆ†æRedditæ•°æ®
  python analysis_main.py --godofprompt      # åªåˆ†æGodOfPromptæ•°æ®
  python analysis_main.py --csv --html       # åˆ†æCSVå’ŒHTMLæ•°æ®
  python analysis_main.py --json --xml --reddit  # åˆ†æJSONã€XMLå’ŒRedditæ•°æ®
  python analysis_main.py --github --godofprompt  # åˆ†æGitHubå’ŒGodOfPromptæ•°æ®
        """
    )

    # æ•°æ®æºé€‰æ‹©å‚æ•°
    parser.add_argument('--json', action='store_true',
                       help='åˆ†æJSONæ•°æ®æº')
    parser.add_argument('--csv', action='store_true',
                       help='åˆ†æCSVæ•°æ®æº')
    parser.add_argument('--xml', action='store_true',
                       help='åˆ†æXMLæ•°æ®æº')
    parser.add_argument('--html', action='store_true',
                       help='åˆ†æHTMLæ•°æ®æº')
    parser.add_argument('--reddit', action='store_true',
                       help='åˆ†æRedditæ•°æ®æº')
    parser.add_argument('--twitter', action='store_true',
                       help='åˆ†æTwitteræ•°æ®æº')
    parser.add_argument('--github', action='store_true',
                       help='åˆ†æGitHubæ•°æ®æº')
    parser.add_argument('--godofprompt', action='store_true',
                       help='åˆ†æGodOfPromptæ•°æ®æº')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--output-dir', type=str,
                       help='Output directoryè·¯å¾„ (é»˜è®¤: testscan_data)')
    parser.add_argument('--lookup-file', type=str,
                       help='æ ‡è¯†ç¬¦çŠ¶æ€æŸ¥æ‰¾è¡¨Fileè·¯å¾„ (é»˜è®¤: identifier_status_lookup.json)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--force-extract', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°æå–å­—ç¬¦ï¼Œå³ä½¿has beenæœ‰æå–ç»“æœ')

    return parser.parse_known_args()

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args, unknown_args = parse_arguments()

    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®æº
    data_sources = []
    if args.json:
        data_sources.append('json')
    if args.csv:
        data_sources.append('csv')
    if args.xml:
        data_sources.append('xml')
    if args.html:
        data_sources.append('html')
    if args.reddit:
        data_sources.append('reddit')
    if args.twitter:
        data_sources.append('twitter')
    if args.github:
        data_sources.append('github')
    if args.godofprompt:
        data_sources.append('godofprompt')

    # å¤„ç†åŠ¨æ€å‚æ•° (ä¾‹å¦‚ --reddit_top)
    for arg in unknown_args:
        if arg.startswith('--'):
            source_name = arg[2:]
            if source_name and source_name not in data_sources:
                data_sources.append(source_name)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ•°æ®æºï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰æ•°æ®æº
    if not data_sources:
        data_sources = ['json', 'csv', 'xml', 'html', 'reddit', 'twitter', 'github', 'godofprompt']

    print("ğŸ” Unicodeæ ‡è¯†ç¬¦çŠ¶æ€åˆ†æå·¥å…·")
    print("="*50)
    print(f"ğŸ“Š åˆ†ææ•°æ®æº: {', '.join(data_sources)}")
    print("="*50)

    # åˆå§‹åŒ–åˆ†æç®¡ç†å™¨
    analysis_manager = UnicodeAnalysisManager(
        output_dir=args.output_dir,
        lookup_file=args.lookup_file,
        data_sources=data_sources,
        force_extract=args.force_extract
    )

    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
    config_stats = analysis_manager.config.get_statistics()
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   âœ… å…è®¸å­—ç¬¦æ•°: {config_stats['total_allowed_characters']:,}")
    print(f"   ğŸ“ æŸ¥æ‰¾è¡¨File: {config_stats['lookup_file']}")
    print(f"   ğŸ”§ é…ç½®çŠ¶æ€: {config_stats['config_status']}")

    # æ‰§è¡Œåˆ†æ
    result = analysis_manager.analyze_unicode_restrictions()

    if "error" in result:
        print(f"\nâŒ åˆ†æFailed: {result['error']}")
        return

    # æ˜¾ç¤ºåˆ†ææ‘˜è¦
    analysis_manager.print_analysis_summary(result)

    # æ˜¾ç¤ºæ£€æµ‹è¯¦æƒ…
    if result["restriction_detection"]["total_restrictions"] > 0:
        base_output_dir = result["analysis_info"]["output_directory"]
        print(f"\nğŸ’¡ å»ºè®®æŸ¥çœ‹è¯¦ç»†æ£€æµ‹ç»“æœ:")

        # æ˜¾ç¤ºå„æ•°æ®æºçš„æ£€æµ‹ç»“æœ
        for source in data_sources:
            source_dir = f"{base_output_dir}/threat_detection_{source}"
            print(f"   ğŸ“ {source.upper()} æ•°æ®æº:")
            print(f"      ğŸ“‹ æ£€æµ‹æ±‡æ€»: {source_dir}/identifier_status_detection_summary.json")
            print(f"      ğŸ” å—é™å­—ç¬¦: {source_dir}/identifier_status_detections_identifier_status.json")

        print(f"   ğŸ“Š æ€»ä½“æ±‡æ€»: {base_output_dir}/identifier_status_detection_overall_summary.json")

    print(f"\nâœ… Unicode identifier status analysis completedï¼")

if __name__ == "__main__":
    main()
