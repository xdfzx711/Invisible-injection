#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ ‡è¯†ç¬¦çŠ¶æ€æ£€æµ‹å™¨
åŸºäº Unicode IdentifierStatus æ ‡å‡†çš„ç®€åŒ–æ£€æµ‹ç³»ç»Ÿ
"""

import json
import time
import unicodedata
from pathlib import Path
from typing import Dict, List, Any, Union
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹directoryåˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥é¡¹ç›®å·¥å…·
from data_collection.utils.logger import setup_logger

# ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜
try:
    from .identifier_status_config import IdentifierStatusConfig
except ImportError:
    from identifier_status_config import IdentifierStatusConfig


class IdentifierStatusDetector:
    """æ ‡è¯†ç¬¦çŠ¶æ€æ£€æµ‹å™¨"""

    def __init__(self, config: IdentifierStatusConfig, output_dir: Union[str, Path] = "testscan_data/unicode_analysis", data_sources: List[str] = None):
        self.config = config
        self.base_output_dir = Path(output_dir)
        self.data_sources = data_sources or ['general']

        # ä¸ºæ¯ä¸ªæ•°æ®æºåˆ›å»ºOutput directory
        self.output_dirs = {}
        for source in self.data_sources:
            source_output_dir = self.base_output_dir / f"threat_detection_{source}"
            source_output_dir.mkdir(parents=True, exist_ok=True)
            self.output_dirs[source] = source_output_dir

        # é»˜è®¤Output directoryï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
        self.output_dir = self.base_output_dir
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger('IdentifierStatusDetector', 'identifier_status_detector.log')
        
        # æ£€æµ‹ç»Ÿè®¡
        self.detection_stats = {
            "total_characters_checked": 0,
            "restricted_characters_found": 0,
            "allowed_characters_found": 0,
            "normalization_issues_found": 0,
            "detection_time": 0.0
        }
    
    def detect_restrictions_in_characters(self, all_characters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å­—ç¬¦åˆ—è¡¨ä¸­çš„å—é™å­—ç¬¦"""
        if not all_characters:
            self.logger.warning("æ²¡æœ‰å­—ç¬¦éœ€è¦æ£€æµ‹")
            return []
        
        self.logger.info(f"å¼€å§‹æ£€æµ‹ {len(all_characters)} ä¸ªå­—ç¬¦çš„æ ‡è¯†ç¬¦çŠ¶æ€")
        start_time = time.time()
        
        # é‡ç½®ç»Ÿè®¡
        self.detection_stats = {
            "total_characters_checked": 0,
            "restricted_characters_found": 0,
            "allowed_characters_found": 0,
            "normalization_issues_found": 0,
            "detection_time": 0.0
        }
        
        detections = []
        
        # æ£€æµ‹æ ‡è¯†ç¬¦çŠ¶æ€
        if self.config.is_detection_enabled("identifier_status"):
            detections.extend(self._detect_identifier_status(all_characters))
        
        # æ£€æµ‹è§„èŒƒåŒ–é—®é¢˜ï¼ˆä¿ç•™ç°æœ‰åŠŸèƒ½ï¼‰
        if self.config.is_detection_enabled("normalization"):
            normalization_detections = self._detect_normalization_issues(all_characters)
            detections.extend(normalization_detections)
            self.detection_stats["normalization_issues_found"] = len(normalization_detections)
        
        end_time = time.time()
        self.detection_stats["detection_time"] = end_time - start_time
        
        self.logger.info(f"æ£€æµ‹Completedï¼Œå‘ç° {len(detections)} ä¸ªé—®é¢˜å­—ç¬¦")
        self.logger.info(f"  - å—é™å­—ç¬¦: {self.detection_stats['restricted_characters_found']}")
        self.logger.info(f"  - è§„èŒƒåŒ–é—®é¢˜: {self.detection_stats['normalization_issues_found']}")
        
        # ä¿å­˜æ£€æµ‹ç»“æœ
        self._save_detection_results(detections)
        
        return detections
    
    def _detect_identifier_status(self, all_characters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ ‡è¯†ç¬¦çŠ¶æ€"""
        detections = []
        
        for char_info in all_characters:
            char = char_info.get("character", "")
            if not char:
                continue
            
            self.detection_stats["total_characters_checked"] += 1
            
            # Checkå­—ç¬¦æ˜¯å¦å—é™
            if self.config.is_character_restricted(char):
                detection = self._create_restriction_detection(char_info)
                detections.append(detection)
                self.detection_stats["restricted_characters_found"] += 1
            else:
                self.detection_stats["allowed_characters_found"] += 1
        
        return detections
    
    def _create_restriction_detection(self, char_info: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå—é™å­—ç¬¦æ£€æµ‹è®°å½•"""
        char = char_info.get("character", "")
        unicode_point = char_info.get("unicode_point", f"U+{ord(char):04X}")
        source_info = char_info.get("source_info", {})
        
        # è·å–å­—ç¬¦çš„åŸºæœ¬ä¿¡æ¯
        try:
            char_name = unicodedata.name(char, f"UNNAMED-{unicode_point}")
            char_category = unicodedata.category(char)
        except ValueError:
            char_name = f"UNNAMED-{unicode_point}"
            char_category = "Cn"
        
        # åˆ›å»ºç®€åŒ–çš„æ£€æµ‹è®°å½•
        detection = {
            "detection_id": f"restricted_{self.detection_stats['restricted_characters_found']:06d}",
            "character": char,
            "unicode_point": unicode_point,
            "status": "Restricted",
            "name": char_name,
            "category": char_category,
            "position_in_string": char_info.get("position_in_string", 0),
            "source_info": {
                "string_value": source_info.get("string_value", ""),
                "file_path": source_info.get("file_path", ""),
                "file_name": source_info.get("file_name", ""),
                "source_type": source_info.get("source_type", ""),
                "field_type": source_info.get("field_type", "")
            },
            "detection_info": {
                "detection_type": "identifier_status",
                "reason": "Character not in Unicode IdentifierStatus allowed list",
                "standard": "Unicode UTS #39",
                "severity": "restriction"
            }
        }
        
        return detection
    
    def _detect_normalization_issues(self, all_characters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹è§„èŒƒåŒ–é—®é¢˜ï¼ˆä¿ç•™ç°æœ‰åŠŸèƒ½çš„ç®€åŒ–ç‰ˆï¼‰"""
        normalization_detections = []
        processed_strings = set()
        
        for char_info in all_characters:
            normalization_info = char_info.get("normalization_info", {})
            
            if not normalization_info:
                continue
            
            # é¿å…é‡å¤å¤„ç†ç›¸åŒçš„å­—ç¬¦ä¸²
            original_string = normalization_info.get("original_string", "")
            if original_string in processed_strings:
                continue
            processed_strings.add(original_string)
            
            # Checkæ˜¯å¦æœ‰è§„èŒƒåŒ–å˜åŒ–
            if normalization_info.get("has_normalization_changes", False):
                detection = self._create_normalization_detection(char_info, normalization_info)
                if detection:
                    normalization_detections.append(detection)
        
        return normalization_detections
    
    def _create_normalization_detection(self, char_info: Dict[str, Any], normalization_info: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºè§„èŒƒåŒ–é—®é¢˜æ£€æµ‹è®°å½•"""
        source_info = char_info.get("source_info", {})
        
        detection = {
            "detection_id": f"normalization_{len(self.detection_stats):06d}",
            "detection_type": "normalization_issue",
            "original_string": normalization_info.get("original_string", ""),
            "normalized_string": normalization_info.get("final_string_used", ""),
            "normalization_changes": normalization_info.get("normalization_changes", []),
            "risk_level": normalization_info.get("normalization_risk_level", "low"),
            "source_info": {
                "file_path": source_info.get("file_path", ""),
                "file_name": source_info.get("file_name", ""),
                "source_type": source_info.get("source_type", ""),
                "field_type": source_info.get("field_type", "")
            },
            "detection_info": {
                "detection_type": "normalization",
                "reason": "String contains characters that change during Unicode normalization",
                "severity": "warning"
            }
        }
        
        return detection
    
    def _save_detection_results(self, detections: List[Dict[str, Any]]):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        if not detections:
            self.logger.info("æ²¡æœ‰æ£€æµ‹ç»“æœéœ€è¦ä¿å­˜")
            return

        # æŒ‰æ•°æ®æºå’Œæ£€æµ‹ç±»å‹åˆ†ç»„ä¿å­˜
        source_detection_groups = {}
        for detection in detections:
            # è·å–æ•°æ®æºä¿¡æ¯
            source_type = detection.get("source_info", {}).get("source_type", "general")
            detection_type = detection.get("detection_info", {}).get("detection_type", "unknown")

            if source_type not in source_detection_groups:
                source_detection_groups[source_type] = {}
            if detection_type not in source_detection_groups[source_type]:
                source_detection_groups[source_type][detection_type] = []

            source_detection_groups[source_type][detection_type].append(detection)

        # ä¸ºæ¯ä¸ªæ•°æ®æºä¿å­˜æ£€æµ‹ç»“æœ
        # æ³¨æ„ï¼šidentifier_status æ¨¡å¼çš„è¯¦ç»†æ£€æµ‹Filehas beenåºŸå¼ƒï¼Œä½¿ç”¨ threat_based æ¨¡å¼çš„ formatted_threats File
        self.logger.info(f"æ£€æµ‹Completedï¼Œå…± {len(detections)} entriesè®°å½•ï¼ˆä½¿ç”¨æ–°æ ¼å¼å¨èƒæŠ¥å‘Šï¼‰")
        
        # æ—§çš„Fileä¿å­˜é€»è¾‘has beenæ³¨é‡Šï¼ˆhas beenåºŸå¼ƒ identifier_status_detections å’Œ identifier_status_detection_summary Fileï¼‰
        # for source_type, detection_groups in source_detection_groups.items():
        #     output_dir = self.output_dirs.get(source_type, self.output_dir)
        #     for detection_type, group_detections in detection_groups.items():
        #         output_file = output_dir / f"identifier_status_detections_{detection_type}.json"
        #         # ... (ä¿å­˜é€»è¾‘has beenç§»é™¤)
        #     source_summary_file = output_dir / "identifier_status_detection_summary.json"
        #     # ... (ä¿å­˜é€»è¾‘has beenç§»é™¤)
    
    def get_detection_statistics(self) -> Dict[str, Any]:
        """è·å–æ£€æµ‹Statistics"""
        return self.detection_stats.copy()


def main():
    """Test function"""
    print("=== æ ‡è¯†ç¬¦çŠ¶æ€æ£€æµ‹å™¨æµ‹è¯• ===\n")
    
    # åˆ›å»ºé…ç½®å’Œæ£€æµ‹å™¨
    config = IdentifierStatusConfig()
    detector = IdentifierStatusDetector(config)
    
    # æ¨¡æ‹Ÿå­—ç¬¦æ•°æ®
    test_characters = [
        {
            "character": "a",
            "unicode_point": "U+0061",
            "position_in_string": 0,
            "source_info": {"string_value": "test", "file_name": "test.txt"}
        },
        {
            "character": "Ğ°",  # è¥¿é‡Œå°”å­—æ¯ a
            "unicode_point": "U+0430",
            "position_in_string": 1,
            "source_info": {"string_value": "test", "file_name": "test.txt"}
        },
        {
            "character": "ğŸ™‚",  # è¡¨æƒ…ç¬¦å·
            "unicode_point": "U+1F642",
            "position_in_string": 2,
            "source_info": {"string_value": "test", "file_name": "test.txt"}
        }
    ]
    
    # æ‰§è¡Œæ£€æµ‹
    detections = detector.detect_restrictions_in_characters(test_characters)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"æ£€æµ‹Completedï¼Œå‘ç° {len(detections)} ä¸ªé—®é¢˜")
    for detection in detections:
        char = detection.get("character", "")
        status = detection.get("status", "")
        detection_type = detection.get("detection_info", {}).get("detection_type", "")
        print(f"  '{char}' ({detection.get('unicode_point', '')}): {status} ({detection_type})")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = detector.get_detection_statistics()
    print(f"\nStatistics:")
    print(f"  æ£€æµ‹å­—ç¬¦æ€»æ•°: {stats['total_characters_checked']}")
    print(f"  å—é™å­—ç¬¦æ•°: {stats['restricted_characters_found']}")
    print(f"  å…è®¸å­—ç¬¦æ•°: {stats['allowed_characters_found']}")
    print(f"  æ£€æµ‹è€—æ—¶: {stats['detection_time']:.3f}ç§’")


if __name__ == "__main__":
    main()
